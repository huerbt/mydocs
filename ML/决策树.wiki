| 决策树 | Nov 11, 2018 |
| Author | 胡轶         |
|        |              |


----
https://blog.csdn.net/csqazwsxedc/article/details/65697652

决策数(Decision Tree)在机器学习中也是比较常见的一种算法，属于监督学习中的一种。

    - 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失值不敏感，可以处理不相关特征数据。

    - 缺点：可能会产生过度匹配的问题。

    - 使用数据类型：数值型和标称型。

一般的，一颗决策树包含一个根节点，若干个内部节点，若干叶节点（叶节点对应决策结果）

每个节点就是对样本属性进行判断，在节点上进行的判断要么是导出最终结论，要么是导出进一步的判定问题，其判断范围是在上次决策结果的限定范围之内。


西瓜书中给出的算法伪代码，指出第8步从 A中选择最优划分属性，是最核心的。如图：

{{file:../images/decisionTree.png}}

理解这一点需要参考下面的案例

    一天，老师问了个问题，只根据头发和声音怎么判断一位同学的性别。 
    为了解决这个问题，同学们马上简单的统计了7位同学的相关特征，数据如下：

   | 头发 | 声音 | 性别 |
   | 长   | 粗   | 男   |
   | 短   | 粗   | 男   |
   | 短   | 粗   | 男   |
   | 长   | 细   | 女   |
   | 短   | 细   | 女   |
   | 短   | 粗   | 女   |
   | 长   | 粗   | 女   |
   | 长   | 粗   | 女   |
   |      |      |      |
   
   1 先根据头发判断，若判断不出，再根据声音判断
   
   {{file:../images/treeA.png}}
   
   2 想先根据声音判断，然后再根据头发来判断
   
   {{file:../images/treeB.png}}
   
   
   同学A和同学B谁的决策树好些？计算机做决策树的时候，面对多个特征，该如何选哪个特征为最佳的划分特征？




*该如何选哪个特征为最佳的划分特征？是决策树的核心问题*
有多个属性，要选哪个属性作为根，会影响数据的复杂度。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。于是我们这么想，如果我们能测量数据的复杂度，对比按不同特征分类后的数据复杂度，若按某一特征分类后复杂度减少的更多，那么这个特征即为最佳分类特征。

用熵（entropy）来表示信息的复杂度，熵越大，则信息越复杂
信息增益(information gain)，表示两个信息熵的差值。

1 使用信息增益确定划分最优属性

    ID3决策树算法就是以信息增益为准则来划分属性。信息增益更大的，区分样本的能力更强，更具有代表性。
    
    信息增益准则对可取值数目较多的属性有所偏好
    
2 使用增益率划分最优属性

    C4.5决策树算法使用增益率来选择划分最优属性。
    (因为信息增益准则对可取值数目较多的属性有所遍好）
    
    增益率准则对可取值数目较少的属性有所偏好,所以
    C4.5算法并不是直接选择增益率最大的，而是先找出信息增益率高于平均水平的属性，再从中选择增益率最高的。
    
    
    
 作业：
 
用算法语言描述构建一棵树的过程

输入：训练集 D={(x1,y1),(x2,y2),...,(xm,ym)}

      属性集 A={a1,a2,....ad}
      
过程：

1 函数 TreeGenerate（D,A)
    
            1.1 生成结点 node
            
            1.2 调用 chooseBestFeatureToSplit 获得最优属性
            
            1.3 迭代属性判断，返回 tree
    
      
2 函数 chooseBestFeatureToSplit() 选择最优分类特征(可用多种方法）
            
            2.11 调用 calcShannonEnt 获得数据的熵
            
            2.2 利用信息增益，或增益率计算最优分类
            
3 函数 calcShannonEnt(dataSet) 计算数据的熵
       
 {{file:../images/informationEntropy.png}}
 

=== 决策树剪枝处理===

决策树容易产生过拟合，所以通过采用认为剪枝的处理降低过拟合风险。

剪枝分为预剪枝 和 后剪枝

预剪枝和后剪枝都需要评估决策树泛化性能是否得到提升，以此来判断是否要做剪枝

预剪枝有可能带来欠拟合风险

后剪枝决策树比预剪枝决策树保留了更多分支，所以后剪枝比预剪枝的欠拟合风险小

后剪枝决策树的泛化能力也更强，但是训练时间的开销会大很多。

=== 连续值的决策树处理 ===

之前讨论的都是基于离散值，如何让决策树也能处理连续值


=== 对缺失值的处理 ===

